{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f0a019",
   "metadata": {},
   "source": [
    "# Data Collection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a805fa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28683e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm import tqdm \n",
    "import usaddress  \n",
    "from functools import partial\n",
    "import tempfile\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "notebooks_folder = os.getcwd() \n",
    "\n",
    "\n",
    "raw_root_folder = os.path.abspath(os.path.join(notebooks_folder, \"..\", \"data\", \"raw\", \"New York City Sales Data\"))\n",
    "interim_root_folder = os.path.abspath( os.path.join(notebooks_folder, \"..\", \"data\", \"interim\", \"New York City Sales Data\"))\n",
    "processed_root_folder = os.path.abspath( os.path.join(notebooks_folder, \"..\", \"data\", \"processed\", \"New York City Sales Data\"))\n",
    "os.makedirs(raw_root_folder, exist_ok=True)\n",
    "os.makedirs(interim_root_folder, exist_ok=True)  \n",
    "os.makedirs(processed_root_folder, exist_ok=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24a665",
   "metadata": {},
   "source": [
    "# NYC Rolling Sales Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd441f3",
   "metadata": {},
   "source": [
    "## Initial Parsing of the Webpage \n",
    "\n",
    "Looking at the HTML layout of the website, we can see that there are `<table>` elements. These contain the rolling sales data. We can parse all the table rows `<tr>` from the webpage, and begin filtering from there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52016156",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/site/finance/property/property-annualized-sales-update.page\"  # replace with your site\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "rows = soup.find_all(\"tr\")\n",
    "\n",
    "for r in rows:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f1cff",
   "metadata": {},
   "source": [
    "## Web Scrapping Script\n",
    "\n",
    "From the html we parsed previously, there are a few things to note \n",
    "\n",
    "1) There is a title change in the `<td>` from 2016 to 2014. These say `<yyyy> New York City` as opposed to `<yyyy> New York City Sales Data`\n",
    "2) From 2003 - 2017, the legacy extension for excell files `.xls` is used. This is changed to `.xlsx` from 2018 - 2024\n",
    "\n",
    "The Data Saving Structure is as follows: \n",
    "```text\n",
    "New York City Sales Data/\n",
    "â”œâ”€â”€ 2003/\n",
    "â”‚   â”œâ”€â”€ Manhattan.xls\n",
    "â”‚   â”œâ”€â”€ Bronx.xls\n",
    "â”‚   â”œâ”€â”€ Brooklyn.xls\n",
    "â”‚   â”œâ”€â”€ Queens.xls\n",
    "â”‚   â””â”€â”€ Staten Island.xls\n",
    "â”œâ”€â”€ 2004/\n",
    "â”‚   â”œâ”€â”€ Manhattan.xls\n",
    "â”‚   â”œâ”€â”€ Bronx.xls\n",
    "â”‚   â”œâ”€â”€ Brooklyn.xls\n",
    "â”‚   â”œâ”€â”€ Queens.xls\n",
    "â”‚   â””â”€â”€ Staten Island.xls\n",
    "...\n",
    "â””â”€â”€ 2018/\n",
    "    â”œâ”€â”€ Manhattan.xlsx\n",
    "    â”œâ”€â”€ Bronx.xlsx\n",
    "    â”œâ”€â”€ Brooklyn.xlsx\n",
    "    â”œâ”€â”€ Queens.xlsx\n",
    "    â””â”€â”€ Staten Island.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b682036",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all(\"tr\")\n",
    "\n",
    "current_year = None\n",
    "\n",
    "for tr in rows:\n",
    "    cells = tr.find_all(\"td\")\n",
    "    if not cells:\n",
    "        continue\n",
    "\n",
    "    text = cells[0].get_text(strip=True)\n",
    "\n",
    "    # Due to table header change in td, we use regex to find the heading and year\n",
    "    match = re.search(r\"(\\d{4})\\s+New\\s+York\\s+City\", text)\n",
    "    if match:\n",
    "        current_year = match.group(1)\n",
    "        year_folder = os.path.join(raw_root_folder, current_year)\n",
    "        os.makedirs(year_folder, exist_ok=True)\n",
    "        print(f\"\\nSaving files for {current_year}\")\n",
    "        continue\n",
    "\n",
    "    if current_year is None:\n",
    "        continue\n",
    "\n",
    "    borough = cells[0].get_text(strip=True)\n",
    "    link_tag = cells[2].find(\"a\") if len(cells) > 2 else None\n",
    "    if not link_tag or not link_tag.has_attr(\"href\"):\n",
    "        continue\n",
    "\n",
    "    excel_href = link_tag[\"href\"]\n",
    "    excel_url = urljoin(url, excel_href)\n",
    "    \n",
    "\n",
    "    filename = f\"{borough}.xlsx\" if \".xlsx\" in excel_url else f\"{borough}.xls\"\n",
    "    filepath = os.path.join(year_folder, filename)\n",
    "\n",
    "    print(f\"Downloading {borough} {current_year} data\")\n",
    "    with requests.get(excel_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"All files downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa278a5f",
   "metadata": {},
   "source": [
    "# NYC Rolling Sales Data Filtering & Unique Address Collection\n",
    "\n",
    "## Filtering Pipeline Summary\n",
    "\n",
    "This cell performs a full cleaning, filtering, and address-preparation pipeline for NYC property sales files (reads from `raw_root_folder`, writes cleaned `.xlsx` files into `interim_root_folder`).\n",
    "\n",
    "### Data Filtering Main steps performed (in order)\n",
    "\n",
    "1. **File discovery & read**\n",
    "   - Iterates year subfolders in `raw_root_folder`.  \n",
    "   - Skips temporary files and non-Excel files (`.xls` / `.xlsx`).\n",
    "   - Reads the file once with `header=None` to detect the header row. The read uses `dtype=object` so columns are not auto-coerced into numeric types.\n",
    "\n",
    "2. **Early illegal-character cleaning**\n",
    "   - Applies `clean_column_name` / `clean_illegal_chars` to object/string cells to remove control characters that will break Excel (`ASCII 0-31`, etc.).\n",
    "   - This prevents `IllegalCharacterError` when saving.\n",
    "\n",
    "3. **Header detection and realignment**\n",
    "   - Normalizes header candidates and matches them against `expected_columns`.\n",
    "   - Re-reads the file once using the detected header row (`header=header_row_idx`) and `dtype=object`, then normalizes column names.\n",
    "\n",
    "4. **Type conversions**\n",
    "   - Converts integer columns (`BLOCK`, `LOT`, `RESIDENTIAL UNITS`, etc.) to pandas `Int64` (nullable integer).\n",
    "   - Converts numeric columns (`LAND SQUARE FEET`, `GROSS SQUARE FEET`, `SALE PRICE`) to numeric (`float`).\n",
    "   - Normalizes string columns (casts to `str`, replaces `\"nan\"` placeholders).\n",
    "   - Parses `SALE DATE` with `pd.to_datetime(..., errors=\"coerce\")`.\n",
    "\n",
    "5. **Residential filtering**\n",
    "   - Normalizes `BUILDING CLASS CATEGORY` strings.\n",
    "   - Keeps only rows that match residential keywords (`FAMILY`, `RENTAL`, `COOP`, `CONDO`, `CONDOP`, `TAX CLASS 1`).\n",
    "\n",
    "6. **Address parsing & cleaning**\n",
    "   - Uses `usaddress` to parse street components and only retains addresses that contain both a house number and a street name.\n",
    "   - Builds `ADDRESS_CLEAN` from parsed components (number + street types/names).\n",
    "\n",
    "7. **Remove non-market transactions**\n",
    "   - Drops rows where `SALE PRICE` â‰¤ 0 (these are typically non-arms-length or administrative transfers, not market sales).\n",
    "\n",
    "8. **Build `FULL_ADDRESS` for geocoding**\n",
    "   - Maps `BOROUGH` â†’ appropriate city label for geocoders (Manhattan â†’ `New York`, Brooklyn â†’ `Brooklyn`, Queens â†’ `Queens`, Bronx â†’ `Bronx`, Staten Island â†’ `Staten Island`).\n",
    "   - Assembles `FULL_ADDRESS` as:\n",
    "     ```\n",
    "     [ADDRESS_CLEAN], [CITY], NY, [ZIP CODE]\n",
    "     ```\n",
    "     Example: `123 MAIN ST, BROOKLYN, NY, 11215`.\n",
    "\n",
    "9. **Column selection**\n",
    "   - Keeps only the minimal columns needed for mapping/visualization:\n",
    "     - `FULL_ADDRESS`  \n",
    "     - `SALE PRICE`  \n",
    "     - `SALE DATE`\n",
    "\n",
    "10. **Save & convert**\n",
    "    - Ensures `interim_root_folder/<year>/` exists and saves cleaned data as `.xlsx` into that folder.\n",
    "    - If the original file was `.xls`, the script removes the old `.xls` after saving the cleaned `.xlsx`.\n",
    "\n",
    "### Final output\n",
    "- For each processed raw file, you get a cleaned `.xlsx` in `interim_root_folder/<year>/` containing only:\n",
    "     - `FULL_ADDRESS`  \n",
    "     - `SALE PRICE`  \n",
    "     - `SALE DATE`\n",
    "\n",
    "## Unique Address Collection \n",
    "\n",
    "After processing and cleaning all raw property sales files, we collect the **unique addresses** for geocoding and mapping purposes.\n",
    "\n",
    "1. **Track unique addresses while processing**\n",
    "   - Each cleaned `FULL_ADDRESS` from the current file is added to a Python `set()` to automatically ensure uniqueness:\n",
    "     ```python\n",
    "     unique_addresses.update(df[\"FULL ADDRESS\"])\n",
    "     ```\n",
    "\n",
    "2. **Convert to DataFrame**\n",
    "   - Convert the set of unique addresses into a sorted pandas DataFrame:\n",
    "     ```python\n",
    "     unique_df = pd.DataFrame(sorted(unique_addresses), columns=[\"FULL ADDRESS\"])\n",
    "     ```\n",
    "\n",
    "3. **Assign round-robin groups**\n",
    "   - Assign each address to a deterministic group (useful for batch processing or parallel geocoding):\n",
    "     ```python\n",
    "     n_groups = 6\n",
    "     unique_df[\"GROUP\"] = (unique_df.index % n_groups) + 1\n",
    "     ```\n",
    "\n",
    "4. **Add blank latitude/longitude columns for future geocoding**\n",
    "   - Prepare columns to store geocoded coordinates later:\n",
    "     ```python\n",
    "     unique_df[\"LAT\"] = None\n",
    "     unique_df[\"LON\"] = None\n",
    "     ```\n",
    "\n",
    "5. **Reorder columns**\n",
    "   - Ensure the final column order is consistent:\n",
    "     ```python\n",
    "     unique_df = unique_df[[\"GROUP\", \"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "     ```\n",
    "\n",
    "6. **Save to Excel**\n",
    "   - Save the unique addresses and groups to an Excel file in the `interim_root_folder`:\n",
    "     ```python\n",
    "     geocode_cache_path = os.path.join(interim_root_folder, \"unique_addresses.xlsx\")\n",
    "     unique_df.to_excel(geocode_cache_path, index=False, engine=\"openpyxl\")\n",
    "     ```\n",
    "\n",
    "### Final Output\n",
    "- `unique_addresses.xlsx` containing:\n",
    "  - `GROUP` â†’ deterministic assignment for batch processing\n",
    "  - `FULL ADDRESS` â†’ cleaned, geocodable addresses\n",
    "  - `LAT` / `LON` â†’ blank for now, to be filled later by geocoding\n",
    "- Ensures **all addresses are unique** across years and files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# config\n",
    "# ==============================\n",
    "expected_columns = [\n",
    "    \"BOROUGH\", \"NEIGHBORHOOD\", \"BUILDING CLASS CATEGORY\", \"TAX CLASS AT PRESENT\",\n",
    "    \"BLOCK\", \"LOT\", \"EASE-MENT\", \"BUILDING CLASS AT PRESENT\", \"ADDRESS\",\n",
    "    \"APARTMENT NUMBER\", \"ZIP CODE\", \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\",\n",
    "    \"TOTAL UNITS\", \"LAND SQUARE FEET\", \"GROSS SQUARE FEET\", \"YEAR BUILT\",\n",
    "    \"TAX CLASS AT TIME OF SALE\", \"BUILDING CLASS AT TIME OF SALE\",\n",
    "    \"SALE PRICE\", \"SALE DATE\"\n",
    "]\n",
    "\n",
    "int_cols = [\"BLOCK\", \"LOT\", \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\", \"TOTAL UNITS\", \"YEAR BUILT\"]\n",
    "float_cols = [\"LAND SQUARE FEET\", \"GROSS SQUARE FEET\", \"SALE PRICE\"]\n",
    "str_cols = [\"BOROUGH\", \"NEIGHBORHOOD\", \"BUILDING CLASS CATEGORY\", \"TAX CLASS AT PRESENT\",\n",
    "            \"EASE-MENT\", \"BUILDING CLASS AT PRESENT\", \"ADDRESS\", \"APARTMENT NUMBER\",\n",
    "            \"ZIP CODE\", \"TAX CLASS AT TIME OF SALE\", \"BUILDING CLASS AT TIME OF SALE\"]\n",
    "datetime_cols = [\"SALE DATE\"]\n",
    "\n",
    "residential_keywords = [\n",
    "    \"FAMILY\", \"RENTAL\", \"COOP\", \"CONDO\", \"CONDOP\", \"TAX CLASS 1\"\n",
    "]\n",
    "\n",
    "borough_encoding__to_city_map = {\"1\": \"New York\", \"2\": \"Bronx\", \"3\": \"Brooklyn\", \"4\": \"Queens\", \"5\": \"Staten Island\"}\n",
    "unique_addresses = set()\n",
    "\n",
    "# ==============================\n",
    "# REGEX CLEANING HELPER FUNCTIONS\n",
    "# ==============================\n",
    "def normalize(col):\n",
    "    return str(col).replace('\\n', ' ').replace('\"', '').replace(\"  \", \" \").strip().upper()\n",
    "\n",
    "def clean_column_name(s):\n",
    "    if isinstance(s, str):\n",
    "        # Remove illegal characters (ASCII 0-31 except \\t, \\n, \\r)\n",
    "        s = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_illegal_chars(val, replace_with=\"\"):\n",
    "    \"\"\"Remove Excel-illegal control chars from a string.\n",
    "       replace_with: '' (remove) or ' ' (replace with space)\n",
    "    \"\"\"\n",
    "    if isinstance(val, str):\n",
    "        # remove ASCII 0-31 and 127-159\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', replace_with, val).strip()\n",
    "    return val\n",
    "\n",
    "# ==============================\n",
    "# PROPERTY TYPE CLEANING\n",
    "# ==============================\n",
    "def is_residential(category: str) -> bool:\n",
    "    \"\"\"Check if a category is residential.\"\"\"\n",
    "    if not isinstance(category, str):\n",
    "        return False\n",
    "    category = category.upper()\n",
    "    return any(k in category for k in residential_keywords)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ADDRESS CLEANING\n",
    "# ==============================\n",
    "def clean_address(addr: str) -> str | None:\n",
    "    \"\"\"Try to parse & clean address. Returns cleaned address or None if invalid.\"\"\"\n",
    "    if not isinstance(addr, str) or not addr.strip():\n",
    "        return None\n",
    "\n",
    "    addr = re.sub(r\"\\s+\", \" \", addr.strip().title())\n",
    "\n",
    "    try:\n",
    "        parsed, _ = usaddress.tag(addr)\n",
    "        # Keep only addresses that have street + house number\n",
    "        if \"AddressNumber\" in parsed and \"StreetName\" in parsed:\n",
    "            # Rebuild normalized street address\n",
    "            parts = [\n",
    "                parsed.get(\"AddressNumber\", \"\"),\n",
    "                parsed.get(\"StreetNamePreType\", \"\"),\n",
    "                parsed.get(\"StreetName\", \"\"),\n",
    "                parsed.get(\"StreetNamePostType\", \"\")\n",
    "            ]\n",
    "            clean = \" \".join([p for p in parts if p]).strip()\n",
    "            return clean\n",
    "        else:\n",
    "            return None\n",
    "    except usaddress.RepeatedLabelError:\n",
    "        return None\n",
    "    \n",
    "# ==============================\n",
    "# FULL ADDRESS ASSEMBLY\n",
    "# ==============================\n",
    "def build_full_address(row):\n",
    "    parts = [row[\"ADDRESS_CLEAN\"].strip().title()]\n",
    "\n",
    "    # Add boroughâ†’city\n",
    "    city = borough_encoding__to_city_map[row[\"BOROUGH\"]].strip().title()\n",
    "    parts.append(row[\"NEIGHBORHOOD\"].strip().title())\n",
    "    parts.append(city)\n",
    "    parts.append(\"NY\")\n",
    "    parts.append(str(row[\"ZIP CODE\"]).strip().title())\n",
    "\n",
    "    return \", \".join([p for p in parts if p])\n",
    "\n",
    "# ==============================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==============================\n",
    "for year in os.listdir(raw_root_folder):\n",
    "    year_folder = os.path.join(raw_root_folder, year)\n",
    "    if not os.path.isdir(year_folder):\n",
    "        continue\n",
    "    print(f\"\\n=== Entering year: {year} ===\")\n",
    "\n",
    "    for file in os.listdir(year_folder):\n",
    "\n",
    "        # skip non-Excel files or temp excel files\n",
    "        if not file.lower().endswith((\".xls\", \".xlsx\")) or file.startswith(\"~\"):  \n",
    "            continue\n",
    "\n",
    "        # Read in file (.xls or .xlsx)\n",
    "        file_path = os.path.join(year_folder, file)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, header=None, engine=\"openpyxl\", dtype=object)\n",
    "        except Exception:\n",
    "            df = pd.read_excel(file_path, header=None, engine=\"xlrd\", dtype=object)\n",
    "\n",
    "        # display(df.head(10))\n",
    "\n",
    "        # Clean illegal characters before anything else\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col] = df[col].apply(clean_column_name)\n",
    "\n",
    "        # Detect header row\n",
    "        normalized_expected = [normalize(c) for c in expected_columns]\n",
    "        header_row_idx = None\n",
    "        for i, row in df.iterrows():\n",
    "            normalized_row = [normalize(c) for c in row.values]\n",
    "            matches = sum(col in normalized_expected for col in normalized_row)\n",
    "            if matches >= len(normalized_expected) * 0.7:\n",
    "                header_row_idx = i\n",
    "                break\n",
    "\n",
    "        # read in file, now with proper header alignment\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, header=header_row_idx, engine=\"openpyxl\", dtype=object)\n",
    "        except Exception:\n",
    "            df = pd.read_excel(file_path, header=header_row_idx, engine=\"xlrd\", dtype=object)\n",
    "        df.columns = [normalize(c) for c in df.columns]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # display(df.head(10))\n",
    "        \n",
    "        # Convert cols to proper data types\n",
    "        for col in int_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        for col in float_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        for col in str_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).replace(\"nan\", \"\")\n",
    "\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        # tracking how much data we are removing\n",
    "        before_count = len(df)\n",
    "        \n",
    "        # Cleaning residential categories\n",
    "        df[\"BUILDING CLASS CATEGORY\"] = (\n",
    "            df[\"BUILDING CLASS CATEGORY\"]\n",
    "            .astype(str)\n",
    "            .apply(lambda x: re.sub(r\"\\s+\", \" \", x.strip()))\n",
    "        )\n",
    "        df = df[df[\"BUILDING CLASS CATEGORY\"].apply(is_residential)]\n",
    "\n",
    "        #  Cleaning address\n",
    "        df[\"ADDRESS_CLEAN\"] = df[\"ADDRESS\"].apply(clean_address)\n",
    "        df = df[df[\"ADDRESS_CLEAN\"].notna()]\n",
    "\n",
    "        # Removing $0 sales\n",
    "        df = df[df['SALE PRICE'] > 0]\n",
    "\n",
    "        # Removing 0 ZIP CODE\n",
    "        df = df[df['ZIP CODE'] != \"0\"]\n",
    "\n",
    "        # Borough â†’ city conversion and full address building\n",
    "        df[\"FULL ADDRESS\"] = df.apply(build_full_address, axis=1)\n",
    "\n",
    "        # Keep only relevant columns for mapping/analysis\n",
    "        keep_cols = [\n",
    "            \"FULL ADDRESS\",\n",
    "            \"SALE PRICE\",\n",
    "            \"SALE DATE\",\n",
    "        ]\n",
    "        df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "        # Print out % of removed data\n",
    "        after_count = len(df)\n",
    "        filtered_out = before_count - after_count\n",
    "        print(f\"{file}: Kept {after_count:,} rows, filtered out {filtered_out:,} ({filtered_out / before_count:.1%})\")\n",
    "\n",
    "        # Create new file path with .xlsx extension for consistent file types\n",
    "        interim_year_folder = os.path.join(interim_root_folder, year)\n",
    "        os.makedirs(interim_year_folder, exist_ok=True)\n",
    "        new_file_path = os.path.join(interim_year_folder, os.path.splitext(file)[0] + \".xlsx\")\n",
    "\n",
    "        # Clean all object/string columns before saving to Excel\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col] = df[col].apply(clean_illegal_chars)\n",
    "\n",
    "        # Tracks unique addresses seen\n",
    "        # Will be used later to get lat and longs\n",
    "        unique_addresses.update(df[\"FULL ADDRESS\"])\n",
    "\n",
    "        # Save back to the same Excel file\n",
    "        df.to_excel(new_file_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Done filtering!\")\n",
    "\n",
    "\n",
    "print(\"Creating address geocache data frame\")\n",
    "# Convert to DataFrame\n",
    "addresses_df = pd.DataFrame(sorted(unique_addresses), columns=[\"FULL ADDRESS\"])\n",
    "\n",
    "# Add blank LAT/LON columns\n",
    "addresses_df[\"LAT\"] = None\n",
    "addresses_df[\"LON\"] = None\n",
    "\n",
    "# Reorder columns\n",
    "addresses_df = addresses_df[[\"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Save CSV\n",
    "geocode_cache_path = os.path.join(interim_root_folder, \"addresses.xlsx\")\n",
    "addresses_df.to_excel(geocode_cache_path, index=False, engine=\"openpyxl\")\n",
    "print(f\"Saved {len(addresses_df)} unique addresses to {geocode_cache_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2108d",
   "metadata": {},
   "source": [
    "## Advanced Address Cleaning & Normalization for Geocoding\n",
    "\n",
    "This step performs a **comprehensive cleaning, normalization, and deduplication** of New York City property addresses. The primary purpose is to **reduce the number of geocoding lookups** by condensing multiple variations of the same address into a single normalized form. Once we have the normalized addresses, we can use them as a **key to append latitude and longitude** to our NYC sales data.\n",
    "\n",
    "### Key Features of the Workflow\n",
    "\n",
    "1. **Initial Cleaning**\n",
    "   - Removes illegal/control characters that can break Excel (`ASCII 0-31`, `\\x7F`).\n",
    "   - Strips stray leading punctuation or brackets (e.g., `(56 Street, Bensonhurst, Brooklyn, NY, 11010)` â†’ `56 Street, Bensonhurst,Brooklyn, NY, 11010`).\n",
    "   - Removes excessive leading zeros in street numbers (e.g., `0000 100th Street` â†’ `100th Street`).\n",
    "   - Normalizes spacing and capitalization.\n",
    "\n",
    "2. **Parsing Components**\n",
    "   - Splits `FULL ADDRESS` into:\n",
    "     - `STREET`\n",
    "     - `CITY`\n",
    "     - `STATE`\n",
    "     - `ZIP`\n",
    "   - This allows fine-grained normalization of street names.\n",
    "\n",
    "3. **Street Name Normalization**\n",
    "   - Maps abbreviations to full street type names:\n",
    "     - `AVE`, `AV`, `AVE.` â†’ `Avenue`\n",
    "     - `ST`, `ST.` â†’ `Street`\n",
    "     - `RD` â†’ `Road`\n",
    "     - `PL` â†’ `Place`\n",
    "     - etc.\n",
    "   - Normalizes street ordinals:\n",
    "     - Numeric-only street numbers like `107` â†’ `107th`\n",
    "     - Correctly handles `1 â†’ 1st`, `2 â†’ 2nd`, `3 â†’ 3rd`, and special cases `11-13 â†’ th`.\n",
    "\n",
    "4. **Normalized Full Address**\n",
    "   - Reconstructs `FULL ADDRESS NORM` as:\n",
    "     ```\n",
    "     [STREET_NORM], [CITY], [STATE], [ZIP]\n",
    "     ```\n",
    "   - Ensures all variations of the same address map to the same normalized form, e.g.:\n",
    "     ```\n",
    "     1 Ascan Ave, Forest Hills, Queens, NY, 11375\n",
    "     1 Ascan Avenue, Forest Hills, Queens, NY, 11375\n",
    "     â†’ 1 Ascan Avenue, Forest Hills, Queens, NY, 11375\n",
    "     ```\n",
    "\n",
    "5. **Detecting Merged Addresses**\n",
    "   - Groups addresses by `FULL ADDRESS NORM`.\n",
    "   - Prints only those groups where **two or more original addresses were mapped to the same normalized address**, allowing us to verify the deduplication process.\n",
    "\n",
    "6. **Deduplication**\n",
    "   - Keeps only **unique normalized addresses**.\n",
    "   - Adds a `GROUP` assignment for round-robin processing (useful for batch geocoding to respect API rate limits).\n",
    "   - Final output contains only the necessary columns:\n",
    "     - `GROUP`\n",
    "     - `FULL ADDRESS` (original, unmodified)\n",
    "     - `FULL ADDRESS NORM`\n",
    "     - `LAT`\n",
    "     - `LON`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de79189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "street_map = {\n",
    "    r\"\\bAVE\\b\\.?\": \"Avenue\",\n",
    "    r\"\\bAV\\b\\.?\": \"Avenue\",\n",
    "    r\"\\bST\\b\\.?\": \"Street\",\n",
    "    r\"\\bRD\\b\\.?\": \"Road\",\n",
    "    r\"\\bPL\\b\\.?\": \"Place\",\n",
    "    r\"\\bTER\\b\\.?\": \"Terrace\",\n",
    "    r\"\\bBLVD\\b\\.?\": \"Boulevard\",\n",
    "    r\"\\bLN\\b\\.?\": \"Lane\",\n",
    "    r\"\\bDR\\b\\.?\": \"Drive\",\n",
    "    r\"\\bCT\\b\\.?\": \"Court\",\n",
    "    r\"\\bHWY\\b\\.?\": \"Highway\",\n",
    "    r\"\\bPKWY\\b\\.?\": \"Parkway\",\n",
    "    r\"\\bSQ\\b\\.?\": \"Square\",\n",
    "    r\"\\bCTR\\b\\.?\": \"Center\",\n",
    "}\n",
    "n_groups = 6\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "def clean_illegal_chars(val):\n",
    "    \"\"\"Remove illegal Excel characters from string cells.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F]', '', val)\n",
    "    return val\n",
    "\n",
    "def clean_address_text(addr: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up weird formatting in addresses:\n",
    "    - Remove stray leading punctuation/brackets.\n",
    "    - Remove excessive leading zeros in street numbers (e.g., 0000 100TH -> 100TH).\n",
    "    - Normalize spaces and capitalization.\n",
    "    \"\"\"\n",
    "    if not isinstance(addr, str):\n",
    "        return addr\n",
    "\n",
    "    # Remove illegal and control characters first\n",
    "    addr = clean_illegal_chars(addr)\n",
    "\n",
    "    # Strip leading punctuation or symbols (like (, \", ', etc.)\n",
    "    addr = re.sub(r'^[^\\w\\d]+', '', addr)\n",
    "\n",
    "    # Fix excessive leading zeros at the beginning of street numbers\n",
    "    # e.g., \"0000 100TH STREET\" -> \"100TH STREET\"\n",
    "    addr = re.sub(r'^\\s*0+\\s*(?=\\d)', '', addr)\n",
    "\n",
    "    # Remove double spaces\n",
    "    addr = re.sub(r'\\s{2,}', ' ', addr)\n",
    "\n",
    "    # Strip trailing/leading whitespace\n",
    "    addr = addr.strip()\n",
    "\n",
    "    # Title formatting for captialization\n",
    "    addr = addr.title()\n",
    "\n",
    "    return addr\n",
    "\n",
    "def normalize_street_ordinal(street_name):\n",
    "    \"\"\"Normalize street ordinals like 107 â†’ 107TH, 1 â†’ 1ST, 2 â†’ 2ND, etc.\"\"\"\n",
    "    tokens = street_name.split()\n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if re.fullmatch(r\"\\d+\", t):  # purely numeric\n",
    "            n = int(t)\n",
    "            if 10 <= n % 100 <= 20:\n",
    "                suffix = \"th\"\n",
    "            else:\n",
    "                suffix = {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\")\n",
    "            new_tokens.append(f\"{t}{suffix}\")\n",
    "        else:\n",
    "            new_tokens.append(t)\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def normalize_street_name(street):\n",
    "    \"\"\"Normalize capitalization, abbreviations, and ordinals in street names.\"\"\"\n",
    "    s = str(street).strip().title()\n",
    "    for pattern, repl in street_map.items():\n",
    "        s = re.sub(pattern, repl, s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = normalize_street_ordinal(s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_full_address(addr):\n",
    "    \"\"\"Parse 'FULL ADDRESS' into [street, neighborhood, city, state, zip].\"\"\"\n",
    "    parts = [p.strip() for p in str(addr).split(\",\")]\n",
    "    if len(parts) != 5:\n",
    "        return [None, None, None, None]\n",
    "    street, neighborhood, city, state, zip_code = parts[0], parts[1], parts[2], parts[3], parts[4]\n",
    "    return [street, neighborhood, city, state, zip_code]\n",
    "\n",
    "# =====================================================\n",
    "# MAIN PROCESSING\n",
    "# =====================================================\n",
    "address_cache_path = os.path.join(interim_root_folder, \"addresses.xlsx\")\n",
    "address_cache_path_remapped = os.path.join(interim_root_folder, \"addresses_condensed.xlsx\")\n",
    "\n",
    "address_df = pd.read_excel(address_cache_path, engine=\"openpyxl\")\n",
    "\n",
    "# Clean Address \n",
    "address_df[\"CLEANED FULL ADDRESS\"] = address_df[\"FULL ADDRESS\"].apply(clean_address_text)\n",
    "\n",
    "# Split into components\n",
    "address_df[[\"STREET\", \"NEIGHBORHOOD\", \"CITY\", \"STATE\", \"ZIP\"]] = address_df[\"CLEANED FULL ADDRESS\"].apply(\n",
    "    lambda x: pd.Series(parse_full_address(x))\n",
    ")\n",
    "address_df[\"STATE\"] = address_df[\"STATE\"].str.upper()\n",
    "\n",
    "# Normalize street names\n",
    "address_df[\"STREET_NORM\"] = address_df[\"STREET\"].apply(normalize_street_name)\n",
    "\n",
    "# Build normalized full address\n",
    "address_df[\"FULL ADDRESS NORM\"] = (\n",
    "    address_df[\"STREET_NORM\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"NEIGHBORHOOD\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"CITY\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"STATE\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"ZIP\"].astype(str)\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# SHOW ONLY MERGED ADDRESSES (duplicates after cleaning)\n",
    "# =====================================================\n",
    "grouped = (\n",
    "    address_df.groupby(\"FULL ADDRESS NORM\")[\"FULL ADDRESS\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "merged = grouped[grouped[\"FULL ADDRESS\"].apply(lambda lst: len(lst) > 1)]\n",
    "\n",
    "\n",
    "print(\"=== ADDRESSES THAT GOT MERGED TO THE SAME NORMALIZED FORM ===\")\n",
    "for _, row in merged.iterrows():\n",
    "    print(f\"\\nâ†’ Normalized: {row['FULL ADDRESS NORM']}\")\n",
    "    for orig in row[\"FULL ADDRESS\"]:\n",
    "        print(f\"   - {orig}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# KEEP ONLY UNIQUE NORMALIZED ADDRESSES\n",
    "# =====================================================\n",
    "unique_address_df = address_df.drop_duplicates(subset=[\"FULL ADDRESS NORM\"]).reset_index(drop=True)\n",
    "\n",
    "# Assign groups round-robin\n",
    "unique_address_df[\"GROUP\"] = (unique_address_df.index % n_groups) + 1\n",
    "\n",
    "# Keeping only neccessary columns\n",
    "unique_address_df = unique_address_df[[\"GROUP\", \"FULL ADDRESS\", \"FULL ADDRESS NORM\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Printing out how much data we re-mapped \n",
    "before_count = len(address_df)\n",
    "after_count = len(unique_address_df)\n",
    "filtered_out = before_count - after_count\n",
    "print(f\"Kept {after_count:,} unique and normalized addresses, condensed down {filtered_out:,} addresses ({filtered_out / before_count:.1%})\")\n",
    "\n",
    "# Save to Excel (includes both columns for traceability)\n",
    "unique_address_df.to_excel(address_cache_path_remapped, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"\\n Saved cleaned, deduplicated addresses to: {address_cache_path_remapped}\")\n",
    "display(unique_address_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46084c7",
   "metadata": {},
   "source": [
    "## Geocoding Pipeline Overview\n",
    "\n",
    "This cell performs **geocoding** of NYC addresses in a safe, restartable, and rate-limited way.  \n",
    "It divides the work into the following main steps:\n",
    "\n",
    "### Config\n",
    "The `config` dictionary defines key parameters:\n",
    "- **`input_file`** â€“ The master Excel file containing all addresses.  \n",
    "- **`group_number`** â€“ The specific group of addresses this run will process.  \n",
    "- **`user_agent`** â€“ Custom identifier for the Nominatim API (required for fair use).  \n",
    "- **`min_delay_seconds`** â€“ Minimum delay between API requests to avoid rate limits.  \n",
    "- **`flush_every`** â€“ How often results are written to disk (in rows).  \n",
    "- **`max_retries`** â€“ Maximum number of retry attempts per address.\n",
    "\n",
    "### Helper Functions\n",
    "1. **`safe_write_excel()`**  \n",
    "   Writes output safely using a temporary file, preventing corruption if the process is interrupted.  \n",
    "\n",
    "2. **`geocode_address()`**  \n",
    "   Calls the Nominatim API with NYC-specific parameters (restricts to U.S. results and English language).  \n",
    "\n",
    "3. **`geocode_with_retries()`**  \n",
    "   Retries failed geocoding requests with exponential backoff (`2, 4, 8... seconds`), up to a maximum retry count.\n",
    "\n",
    "### Main Script Logic\n",
    "\n",
    "1. **Load Data**\n",
    "   - Reads the input Excel file and filters only the addresses for the current group.\n",
    "\n",
    "2. **Initialize Geocoder**\n",
    "   - Creates a `Nominatim` geolocator with a `RateLimiter` to ensure polite API usage.\n",
    "\n",
    "3. **Load Cache**\n",
    "   - If a per-group cache file already exists (`geocode_group_X.xlsx`), it loads it.\n",
    "   - This allows the script to resume progress without re-requesting already geocoded addresses.\n",
    "\n",
    "4. **Build Lookup Set**\n",
    "   - A set of all normalized addresses (`FULL ADDRESS NORM`) already geocoded is created.\n",
    "   - If an address is in this cache, it is skipped automatically.\n",
    "\n",
    "5. **Iterate and Geocode**\n",
    "   - For each new (uncached) address:\n",
    "     - Calls `geocode_with_retries()`.\n",
    "     - On success, records latitude and longitude.\n",
    "     - On failure, records `None` for both coordinates.\n",
    "\n",
    "6. **Flush Periodically**\n",
    "   - Every `flush_every` rows, the script writes results safely to disk using `safe_write_excel()`.\n",
    "   - This ensures progress is not lost if the kernel stops mid-run.\n",
    "\n",
    "7. **Final Flush**\n",
    "   - After all rows are processed, any remaining results are written to the cache file.\n",
    "\n",
    "### Output\n",
    "Each run creates or updates:\n",
    "\n",
    "`interim/geocode_group_<group_number>.xlsx`\n",
    "\n",
    "This file stores all geocoded results for that group and allows future runs to skip completed addresses.\n",
    "\n",
    "- Final output contains only the necessary columns:\n",
    "     - `GROUP`\n",
    "     - `FULL ADDRESS` (original, unmodified)\n",
    "     - `FULL ADDRESS NORM`\n",
    "     - `LAT` \n",
    "     - `LON`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# config\n",
    "# =====================================================\n",
    "config = {\n",
    "    \"input_file\": \"addresses_condensed.xlsx\",\n",
    "    \"group_number\": 1,  # <-- each team member sets their group\n",
    "    \"user_agent\": \"nyc_affordability_transit_access\",\n",
    "    \"min_delay_seconds\": 1.0,\n",
    "    \"flush_every\": 500,  # how many rows before writing to disk\n",
    "    \"max_retries\": 3,\n",
    "}\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def safe_write_excel(df, final_path):\n",
    "    \"\"\"Safely write Excel file without risk of corruption on interruption.\"\"\"\n",
    "    tmp_dir = tempfile.gettempdir()\n",
    "    tmp_path = os.path.join(tmp_dir, f\"tmp_{os.path.basename(final_path)}\")\n",
    "    try:\n",
    "        # Write to a temporary file first\n",
    "        df.to_excel(tmp_path, index=False, engine=\"openpyxl\")\n",
    "        # Atomically replace the old file\n",
    "        shutil.move(tmp_path, final_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to safely write {final_path}: {e}\")\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "\n",
    "def geocode_address(geolocator, address_json):\n",
    "    \"\"\"Call Nominatim geocode with NYC-focused parameters.\"\"\"\n",
    "    return geolocator.geocode(\n",
    "        address_json,\n",
    "        country_codes=\"us\",         # restrict to USA\n",
    "        addressdetails=True,        # return structured address info\n",
    "        language=\"en\",              # English results\n",
    "    )\n",
    "\n",
    "def geocode_with_retries(geocode_func, address, max_retries=3):\n",
    "    \"\"\"Take a single comma-separated address string, and retry if failed.\"\"\"\n",
    "    parts = [p.strip() for p in address.split(\",\")]\n",
    "    address = f\"{parts[0]}, {parts[3]}, {parts[4]}\"\n",
    "    attempt = 0\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            return geocode_func(address)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Warn: geocode error ({attempt}/{max_retries}) for '{address}': {e}. Backing off {wait}s.\")\n",
    "            time.sleep(wait)\n",
    "    return None\n",
    "\n",
    "# =====================================================\n",
    "# MAIN SCRIPT\n",
    "# =====================================================\n",
    "input_path = os.path.join(interim_root_folder, config[\"input_file\"])\n",
    "df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "\n",
    "# Filter to only the given group\n",
    "to_geocode_df = df[(df[\"GROUP\"] == config[\"group_number\"])].reset_index(drop=True)\n",
    "\n",
    "print(f\"Group {config['group_number']} has {len(to_geocode_df)} addresses to geocode.\")\n",
    "\n",
    "# Initialize geolocator with NYC-specific parameters\n",
    "geolocator = Nominatim(user_agent=config[\"user_agent\"], timeout=10)\n",
    "\n",
    "# Use a partial so each call applies NYC bounding box parameters\n",
    "geocode = RateLimiter(\n",
    "    partial(geocode_address, geolocator),\n",
    "    min_delay_seconds=config[\"min_delay_seconds\"],\n",
    "    error_wait_seconds=10\n",
    ")\n",
    "\n",
    "# Prepare per-group output file\n",
    "group_cache_file = os.path.join(\n",
    "    interim_root_folder, f\"geocode_group_{config['group_number']}.xlsx\"\n",
    ")\n",
    "\n",
    "# Load existing cache if available\n",
    "if os.path.exists(group_cache_file):\n",
    "    cache_df = pd.read_excel(group_cache_file, engine=\"openpyxl\")\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=[\"GROUP\",\"FULL ADDRESS\",\"FULL ADDRESS NORM\",\"LAT\",\"LON\"])\n",
    "\n",
    "# Build a set of already geocoded addresses (FULL ADDRESS NORM) for quick lookup\n",
    "cache_key = set(cache_df[\"FULL ADDRESS NORM\"].astype(str).tolist())\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "for idx, row in tqdm(to_geocode_df.iterrows(), total=len(to_geocode_df), desc=\"Geocoding\"):\n",
    "    norm_addr = str(row[\"FULL ADDRESS NORM\"])\n",
    "    \n",
    "    if norm_addr in cache_key:\n",
    "        continue  # already geocoded\n",
    "    \n",
    "    try:\n",
    "        loc = geocode_with_retries(lambda a: geocode(a), norm_addr, max_retries=config[\"max_retries\"])\n",
    "        if loc:\n",
    "            lat, lon = loc.latitude, loc.longitude\n",
    "        else: \n",
    "            lat, lon = None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding '{norm_addr}': {e}\")\n",
    "        lat, lon = None, None\n",
    "\n",
    "    new_rows.append({\n",
    "        \"GROUP\": row[\"GROUP\"],\n",
    "        \"FULL ADDRESS\": row[\"FULL ADDRESS\"],\n",
    "        \"FULL ADDRESS NORM\": row[\"FULL ADDRESS NORM\"],\n",
    "        \"LAT\": lat,\n",
    "        \"LON\": lon,\n",
    "    })\n",
    "\n",
    "    # Flush periodically\n",
    "    if len(new_rows) >= config[\"flush_every\"]:\n",
    "        tmp_df = pd.DataFrame(new_rows)\n",
    "        cache_df = pd.concat([cache_df, tmp_df], ignore_index=True)\n",
    "        safe_write_excel(cache_df, group_cache_file)\n",
    "        print(f\"Flushed {len(new_rows)} rows to {group_cache_file} (total cached: {len(cache_df)})\")\n",
    "        new_rows = []\n",
    "\n",
    "# Final flush\n",
    "if new_rows:\n",
    "    tmp_df = pd.DataFrame(new_rows)\n",
    "    cache_df = pd.concat([cache_df, tmp_df], ignore_index=True)\n",
    "    safe_write_excel(cache_df, group_cache_file)\n",
    "    print(f\"Final flush: {len(new_rows)} rows written to {group_cache_file} (total cached: {len(cache_df)})\")\n",
    "\n",
    "print(f\"\\nGeocoding complete for group {config['group_number']}. Saved results to: {group_cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e26003",
   "metadata": {},
   "source": [
    "## Combining Geocode Files back into original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e973edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging geocode_group_1.xlsx...\n",
      "Merging geocode_group_2.xlsx...\n",
      "Merging geocode_group_3.xlsx...\n",
      "Merging geocode_group_4.xlsx...\n",
      "Merging geocode_group_5.xlsx...\n",
      "Merging geocode_group_6.xlsx...\n",
      "âœ… Done! Saved merged file (non-empty LAT/LON only) to:\n",
      "/Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/interim/New York City Sales Data/addresses_with_latlon.xlsx\n",
      "ðŸ“¦ Final row count: 266648\n"
     ]
    }
   ],
   "source": [
    "# Base folder path\n",
    "interim_root_folder = os.path.abspath(\n",
    "    os.path.join(notebooks_folder, \"..\", \"data\", \"interim\", \"New York City Sales Data\")\n",
    ")\n",
    "\n",
    "# Base addresses file\n",
    "base_path = os.path.join(interim_root_folder, \"addresses_condensed.xlsx\")\n",
    "base = pd.read_excel(base_path)[[\"GROUP\", \"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Find all geocode group files\n",
    "group_files = sorted(glob.glob(os.path.join(interim_root_folder, \"geocode_group_*.xlsx\")))\n",
    "\n",
    "# Merge each group file\n",
    "for file in group_files:\n",
    "    print(f\"Merging {os.path.basename(file)}...\")\n",
    "    group_df = pd.read_excel(file)\n",
    "    \n",
    "    # Merge LAT/LON from each geocode file\n",
    "    base = base.merge(\n",
    "        group_df[[\"GROUP\", \"FULL ADDRESS\", \"LAT\", \"LON\"]],\n",
    "        on=[\"GROUP\", \"FULL ADDRESS\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_y\")\n",
    "    )\n",
    "    \n",
    "    # Fill missing LAT/LON values\n",
    "    base[\"LAT\"] = base[\"LAT\"].fillna(base[\"LAT_y\"])\n",
    "    base[\"LON\"] = base[\"LON\"].fillna(base[\"LON_y\"])\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    base.drop(columns=[\"LAT_y\", \"LON_y\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Drop rows with missing LAT or LON\n",
    "base = base.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "# Keep only final columns\n",
    "final = base[[\"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Save to Excel\n",
    "output_path = os.path.join(interim_root_folder, \"addresses_with_latlon.xlsx\")\n",
    "final.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Done! Saved merged file (non-empty LAT/LON only) to:\\n{output_path}\")\n",
    "print(f\"ðŸ“¦ Final row count: {len(final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfd3fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 266648 address records with LAT/LON data.\n",
      "\n",
      "=== Entering year: 2013 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2685 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3186 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (595 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1656 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12029 rows kept).\n",
      "\n",
      "=== Entering year: 2014 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2572 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3118 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (632 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1818 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (11999 rows kept).\n",
      "\n",
      "=== Entering year: 2022 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (3264 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3656 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (827 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2301 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (13704 rows kept).\n",
      "\n",
      "=== Entering year: 2024 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2552 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2884 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (634 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1818 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (11064 rows kept).\n",
      "\n",
      "=== Entering year: 2023 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2599 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2636 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (629 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1719 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (10636 rows kept).\n",
      "\n",
      "=== Entering year: 2015 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2675 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3165 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (719 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2074 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12441 rows kept).\n",
      "\n",
      "=== Entering year: 2012 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2318 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2888 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (544 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1296 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (10113 rows kept).\n",
      "\n",
      "=== Entering year: 2008 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2951 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2843 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (664 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1623 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12514 rows kept).\n",
      "\n",
      "=== Entering year: 2006 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2993 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (4125 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (1126 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2529 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (19009 rows kept).\n",
      "\n",
      "=== Entering year: 2007 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (3380 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3643 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (1030 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2177 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (15783 rows kept).\n",
      "\n",
      "=== Entering year: 2009 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2016 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2257 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (510 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1574 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (11215 rows kept).\n",
      "\n",
      "=== Entering year: 2017 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2029 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3023 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (852 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2582 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (13102 rows kept).\n",
      "\n",
      "=== Entering year: 2010 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2015 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2746 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (487 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1457 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (10247 rows kept).\n",
      "\n",
      "=== Entering year: 2019 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2465 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2970 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (886 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2145 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12249 rows kept).\n",
      "\n",
      "=== Entering year: 2021 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (3126 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3981 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (882 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2752 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (14538 rows kept).\n",
      "\n",
      "=== Entering year: 2020 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (1685 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2392 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (628 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1962 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (9752 rows kept).\n",
      "\n",
      "=== Entering year: 2018 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (1814 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2856 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (857 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2467 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12780 rows kept).\n",
      "\n",
      "=== Entering year: 2011 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2101 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (2853 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (452 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (1015 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (9666 rows kept).\n",
      "\n",
      "=== Entering year: 2016 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (2139 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (3001 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (783 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (2338 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (12713 rows kept).\n",
      "\n",
      "=== Entering year: 2005 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (3011 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (4899 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (1232 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (3138 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (21409 rows kept).\n",
      "\n",
      "=== Entering year: 2003 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (3592 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (4327 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (1070 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (3069 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (6312 rows kept).\n",
      "\n",
      "=== Entering year: 2004 ===\n",
      "Processing Manhattan.xlsx...\n",
      "Overwrote Manhattan.xlsx with LAT/LON data (4556 rows kept).\n",
      "Processing Brooklyn.xlsx...\n",
      "Overwrote Brooklyn.xlsx with LAT/LON data (4976 rows kept).\n",
      "Processing Bronx.xlsx...\n",
      "Overwrote Bronx.xlsx with LAT/LON data (1210 rows kept).\n",
      "Processing Staten Island.xlsx...\n",
      "Overwrote Staten Island.xlsx with LAT/LON data (3351 rows kept).\n",
      "Processing Queens.xlsx...\n",
      "Overwrote Queens.xlsx with LAT/LON data (16388 rows kept).\n",
      "\n",
      "All files updated in place with LAT/LON successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path setup\n",
    "latlon_path = os.path.join(interim_root_folder, \"addresses_with_latlon.xlsx\")\n",
    "latlon_df = pd.read_excel(latlon_path)[[\"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "\n",
    "print(f\"Loaded {len(latlon_df)} address records with LAT/LON data.\")\n",
    "\n",
    "for year in os.listdir(interim_root_folder):\n",
    "    year_folder = os.path.join(interim_root_folder, year)\n",
    "    if not os.path.isdir(year_folder):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Entering year: {year} ===\")\n",
    "\n",
    "    for file in os.listdir(year_folder):\n",
    "        # Skip non-Excel or temp Excel files\n",
    "        if not file.lower().endswith((\".xls\", \".xlsx\")) or file.startswith(\"~\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(year_folder, file)\n",
    "\n",
    "        print(f\"Processing {file}...\")\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        if \"FULL ADDRESS\" not in df.columns:\n",
    "            print(f\"Skipping {file} (no 'FULL ADDRESS' column found)\")\n",
    "            continue\n",
    "\n",
    "        # Merge LAT/LON from master list\n",
    "        merged = df.merge(latlon_df, on=\"FULL ADDRESS\", how=\"left\")\n",
    "\n",
    "        # Drop rows where no coordinates were found\n",
    "        merged = merged.dropna(subset=[\"LAT\", \"LON\"])\n",
    "\n",
    "        # Overwrite same file\n",
    "        merged.to_excel(file_path, index=False)\n",
    "        print(f\"Overwrote {file} with LAT/LON data ({len(merged)} rows kept).\")\n",
    "\n",
    "print(\"\\nAll files updated in place with LAT/LON successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9300e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Define core paths\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "data_root = os.path.join(project_root, \"data\")\n",
    "interim_root = os.path.join(data_root, \"interim\", \"New York City Sales Data\")\n",
    "external_root = os.path.join(data_root, \"external\")\n",
    "processed_root = os.path.join(data_root, \"processed\")\n",
    "\n",
    "def compute_nearest_subway_distances():\n",
    "    \"\"\"Compute distance (in meters) from each property to its nearest subway station.\"\"\"\n",
    "\n",
    "    # Load subway stops\n",
    "    stops_path = os.path.join(project_root, \"data\", \"external\", \"gtfs\", \"stops.txt\")\n",
    "    stops_df = pd.read_csv(stops_path)\n",
    "    stops_df = stops_df.dropna(subset=[\"stop_lat\", \"stop_lon\"])\n",
    "    subway_coords = np.array(list(zip(stops_df[\"stop_lat\"], stops_df[\"stop_lon\"])))\n",
    "\n",
    "    # Build KDTree for efficient spatial lookup\n",
    "    subway_tree = cKDTree(subway_coords)\n",
    "    print(f\"Loaded {len(stops_df)} subway stops.\")\n",
    "\n",
    "    # Prepare output folder\n",
    "    os.makedirs(processed_root, exist_ok=True)\n",
    "\n",
    "    # Loop over yearly folders\n",
    "    for year in os.listdir(interim_root):\n",
    "        year_folder = os.path.join(interim_root, year)\n",
    "        if not os.path.isdir(year_folder):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Processing {year} ===\")\n",
    "\n",
    "        for file in os.listdir(year_folder):\n",
    "            if not file.lower().endswith((\".xls\", \".xlsx\")) or file.startswith(\"~\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(year_folder, file)\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "            if not {\"LAT\", \"LON\"}.issubset(df.columns):\n",
    "                print(f\"Skipping {file} (missing LAT/LON).\")\n",
    "                continue\n",
    "\n",
    "            property_coords = np.array(list(zip(df[\"LAT\"], df[\"LON\"])))\n",
    "\n",
    "            # Query nearest subway for each property\n",
    "            dist_deg, idx = subway_tree.query(property_coords, k=1)\n",
    "            df[\"nearest_subway_dist_m\"] = dist_deg * 111_000  # convert degrees â†’ meters\n",
    "\n",
    "            # Add subway stop name (optional)\n",
    "            df[\"nearest_subway_name\"] = stops_df.iloc[idx][\"stop_name\"].values\n",
    "\n",
    "            # Save to processed folder\n",
    "            out_folder = os.path.join(processed_root, year)\n",
    "            os.makedirs(out_folder, exist_ok=True)\n",
    "            out_path = os.path.join(out_folder, f\"{os.path.splitext(file)[0]}_with_subway.xlsx\")\n",
    "\n",
    "            df.to_excel(out_path, index=False)\n",
    "            print(f\"âœ… Saved {out_path} ({len(df)} rows)\")\n",
    "\n",
    "    print(\"\\nAll yearly files updated with nearest subway distance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb1104d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1497 subway stops.\n",
      "\n",
      "=== Processing 2013 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2013/Manhattan_with_subway.xlsx (2685 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2013/Brooklyn_with_subway.xlsx (3186 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2013/Bronx_with_subway.xlsx (595 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2013/Staten Island_with_subway.xlsx (1656 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2013/Queens_with_subway.xlsx (12029 rows)\n",
      "\n",
      "=== Processing 2014 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2014/Manhattan_with_subway.xlsx (2572 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2014/Brooklyn_with_subway.xlsx (3118 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2014/Bronx_with_subway.xlsx (632 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2014/Staten Island_with_subway.xlsx (1818 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2014/Queens_with_subway.xlsx (11999 rows)\n",
      "\n",
      "=== Processing 2022 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2022/Manhattan_with_subway.xlsx (3264 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2022/Brooklyn_with_subway.xlsx (3656 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2022/Bronx_with_subway.xlsx (827 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2022/Staten Island_with_subway.xlsx (2301 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2022/Queens_with_subway.xlsx (13704 rows)\n",
      "\n",
      "=== Processing 2024 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2024/Manhattan_with_subway.xlsx (2552 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2024/Brooklyn_with_subway.xlsx (2884 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2024/Bronx_with_subway.xlsx (634 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2024/Staten Island_with_subway.xlsx (1818 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2024/Queens_with_subway.xlsx (11064 rows)\n",
      "\n",
      "=== Processing 2023 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2023/Manhattan_with_subway.xlsx (2599 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2023/Brooklyn_with_subway.xlsx (2636 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2023/Bronx_with_subway.xlsx (629 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2023/Staten Island_with_subway.xlsx (1719 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2023/Queens_with_subway.xlsx (10636 rows)\n",
      "\n",
      "=== Processing 2015 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2015/Manhattan_with_subway.xlsx (2675 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2015/Brooklyn_with_subway.xlsx (3165 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2015/Bronx_with_subway.xlsx (719 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2015/Staten Island_with_subway.xlsx (2074 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2015/Queens_with_subway.xlsx (12441 rows)\n",
      "\n",
      "=== Processing 2012 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2012/Manhattan_with_subway.xlsx (2318 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2012/Brooklyn_with_subway.xlsx (2888 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2012/Bronx_with_subway.xlsx (544 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2012/Staten Island_with_subway.xlsx (1296 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2012/Queens_with_subway.xlsx (10113 rows)\n",
      "\n",
      "=== Processing 2008 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2008/Manhattan_with_subway.xlsx (2951 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2008/Brooklyn_with_subway.xlsx (2843 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2008/Bronx_with_subway.xlsx (664 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2008/Staten Island_with_subway.xlsx (1623 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2008/Queens_with_subway.xlsx (12514 rows)\n",
      "\n",
      "=== Processing 2006 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2006/Manhattan_with_subway.xlsx (2993 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2006/Brooklyn_with_subway.xlsx (4125 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2006/Bronx_with_subway.xlsx (1126 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2006/Staten Island_with_subway.xlsx (2529 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2006/Queens_with_subway.xlsx (19009 rows)\n",
      "\n",
      "=== Processing 2007 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2007/Manhattan_with_subway.xlsx (3380 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2007/Brooklyn_with_subway.xlsx (3643 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2007/Bronx_with_subway.xlsx (1030 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2007/Staten Island_with_subway.xlsx (2177 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2007/Queens_with_subway.xlsx (15783 rows)\n",
      "\n",
      "=== Processing 2009 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2009/Manhattan_with_subway.xlsx (2016 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2009/Brooklyn_with_subway.xlsx (2257 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2009/Bronx_with_subway.xlsx (510 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2009/Staten Island_with_subway.xlsx (1574 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2009/Queens_with_subway.xlsx (11215 rows)\n",
      "\n",
      "=== Processing 2017 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2017/Manhattan_with_subway.xlsx (2029 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2017/Brooklyn_with_subway.xlsx (3023 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2017/Bronx_with_subway.xlsx (852 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2017/Staten Island_with_subway.xlsx (2582 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2017/Queens_with_subway.xlsx (13102 rows)\n",
      "\n",
      "=== Processing 2010 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2010/Manhattan_with_subway.xlsx (2015 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2010/Brooklyn_with_subway.xlsx (2746 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2010/Bronx_with_subway.xlsx (487 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2010/Staten Island_with_subway.xlsx (1457 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2010/Queens_with_subway.xlsx (10247 rows)\n",
      "\n",
      "=== Processing 2019 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2019/Manhattan_with_subway.xlsx (2465 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2019/Brooklyn_with_subway.xlsx (2970 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2019/Bronx_with_subway.xlsx (886 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2019/Staten Island_with_subway.xlsx (2145 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2019/Queens_with_subway.xlsx (12249 rows)\n",
      "\n",
      "=== Processing 2021 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2021/Manhattan_with_subway.xlsx (3126 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2021/Brooklyn_with_subway.xlsx (3981 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2021/Bronx_with_subway.xlsx (882 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2021/Staten Island_with_subway.xlsx (2752 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2021/Queens_with_subway.xlsx (14538 rows)\n",
      "\n",
      "=== Processing 2020 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2020/Manhattan_with_subway.xlsx (1685 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2020/Brooklyn_with_subway.xlsx (2392 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2020/Bronx_with_subway.xlsx (628 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2020/Staten Island_with_subway.xlsx (1962 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2020/Queens_with_subway.xlsx (9752 rows)\n",
      "\n",
      "=== Processing 2018 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2018/Manhattan_with_subway.xlsx (1814 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2018/Brooklyn_with_subway.xlsx (2856 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2018/Bronx_with_subway.xlsx (857 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2018/Staten Island_with_subway.xlsx (2467 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2018/Queens_with_subway.xlsx (12780 rows)\n",
      "\n",
      "=== Processing 2011 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2011/Manhattan_with_subway.xlsx (2101 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2011/Brooklyn_with_subway.xlsx (2853 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2011/Bronx_with_subway.xlsx (452 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2011/Staten Island_with_subway.xlsx (1015 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2011/Queens_with_subway.xlsx (9666 rows)\n",
      "\n",
      "=== Processing 2016 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2016/Manhattan_with_subway.xlsx (2139 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2016/Brooklyn_with_subway.xlsx (3001 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2016/Bronx_with_subway.xlsx (783 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2016/Staten Island_with_subway.xlsx (2338 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2016/Queens_with_subway.xlsx (12713 rows)\n",
      "\n",
      "=== Processing 2005 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2005/Manhattan_with_subway.xlsx (3011 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2005/Brooklyn_with_subway.xlsx (4899 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2005/Bronx_with_subway.xlsx (1232 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2005/Staten Island_with_subway.xlsx (3138 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2005/Queens_with_subway.xlsx (21409 rows)\n",
      "\n",
      "=== Processing 2003 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2003/Manhattan_with_subway.xlsx (3592 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2003/Brooklyn_with_subway.xlsx (4327 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2003/Bronx_with_subway.xlsx (1070 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2003/Staten Island_with_subway.xlsx (3069 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2003/Queens_with_subway.xlsx (6312 rows)\n",
      "\n",
      "=== Processing 2004 ===\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2004/Manhattan_with_subway.xlsx (4556 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2004/Brooklyn_with_subway.xlsx (4976 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2004/Bronx_with_subway.xlsx (1210 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2004/Staten Island_with_subway.xlsx (3351 rows)\n",
      "âœ… Saved /Users/nayar/Documents/GT/FA25/CSE6242/CSE6242-Project/NYC-Affordability-Transit-Access/data/processed/2004/Queens_with_subway.xlsx (16388 rows)\n",
      "\n",
      "All yearly files updated with nearest subway distance!\n"
     ]
    }
   ],
   "source": [
    "compute_nearest_subway_distances()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NYC-Affordability-Transit-Access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
