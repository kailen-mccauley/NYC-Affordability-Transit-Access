{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f0a019",
   "metadata": {},
   "source": [
    "# Data Collection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a805fa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28683e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm import tqdm \n",
    "import usaddress  \n",
    "from functools import partial\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "notebooks_folder = os.getcwd() \n",
    "\n",
    "\n",
    "raw_root_folder = os.path.abspath(os.path.join(notebooks_folder, \"..\", \"data\", \"raw\", \"New York City Sales Data\"))\n",
    "interim_root_folder = os.path.abspath( os.path.join(notebooks_folder, \"..\", \"data\", \"interim\", \"New York City Sales Data\"))\n",
    "processed_root_folder = os.path.abspath( os.path.join(notebooks_folder, \"..\", \"data\", \"processed\", \"New York City Sales Data\"))\n",
    "os.makedirs(raw_root_folder, exist_ok=True)\n",
    "os.makedirs(interim_root_folder, exist_ok=True)  \n",
    "os.makedirs(processed_root_folder, exist_ok=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24a665",
   "metadata": {},
   "source": [
    "# NYC Rolling Sales Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd441f3",
   "metadata": {},
   "source": [
    "## Initial Parsing of the Webpage \n",
    "\n",
    "Looking at the HTML layout of the website, we can see that there are `<table>` elements. These contain the rolling sales data. We can parse all the table rows `<tr>` from the webpage, and begin filtering from there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52016156",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nyc.gov/site/finance/property/property-annualized-sales-update.page\"  # replace with your site\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "rows = soup.find_all(\"tr\")\n",
    "\n",
    "for r in rows:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f1cff",
   "metadata": {},
   "source": [
    "## Web Scrapping Script\n",
    "\n",
    "From the html we parsed previously, there are a few things to note \n",
    "\n",
    "1) There is a title change in the `<td>` from 2016 to 2014. These say `<yyyy> New York City` as opposed to `<yyyy> New York City Sales Data`\n",
    "2) From 2003 - 2017, the legacy extension for excell files `.xls` is used. This is changed to `.xlsx` from 2018 - 2024\n",
    "\n",
    "The Data Saving Structure is as follows: \n",
    "```text\n",
    "New York City Sales Data/\n",
    "├── 2003/\n",
    "│   ├── Manhattan.xls\n",
    "│   ├── Bronx.xls\n",
    "│   ├── Brooklyn.xls\n",
    "│   ├── Queens.xls\n",
    "│   └── Staten Island.xls\n",
    "├── 2004/\n",
    "│   ├── Manhattan.xls\n",
    "│   ├── Bronx.xls\n",
    "│   ├── Brooklyn.xls\n",
    "│   ├── Queens.xls\n",
    "│   └── Staten Island.xls\n",
    "...\n",
    "└── 2018/\n",
    "    ├── Manhattan.xlsx\n",
    "    ├── Bronx.xlsx\n",
    "    ├── Brooklyn.xlsx\n",
    "    ├── Queens.xlsx\n",
    "    └── Staten Island.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b682036",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all(\"tr\")\n",
    "\n",
    "current_year = None\n",
    "\n",
    "for tr in rows:\n",
    "    cells = tr.find_all(\"td\")\n",
    "    if not cells:\n",
    "        continue\n",
    "\n",
    "    text = cells[0].get_text(strip=True)\n",
    "\n",
    "    # Due to table header change in td, we use regex to find the heading and year\n",
    "    match = re.search(r\"(\\d{4})\\s+New\\s+York\\s+City\", text)\n",
    "    if match:\n",
    "        current_year = match.group(1)\n",
    "        year_folder = os.path.join(raw_root_folder, current_year)\n",
    "        os.makedirs(year_folder, exist_ok=True)\n",
    "        print(f\"\\nSaving files for {current_year}\")\n",
    "        continue\n",
    "\n",
    "    if current_year is None:\n",
    "        continue\n",
    "\n",
    "    borough = cells[0].get_text(strip=True)\n",
    "    link_tag = cells[2].find(\"a\") if len(cells) > 2 else None\n",
    "    if not link_tag or not link_tag.has_attr(\"href\"):\n",
    "        continue\n",
    "\n",
    "    excel_href = link_tag[\"href\"]\n",
    "    excel_url = urljoin(url, excel_href)\n",
    "    \n",
    "\n",
    "    filename = f\"{borough}.xlsx\" if \".xlsx\" in excel_url else f\"{borough}.xls\"\n",
    "    filepath = os.path.join(year_folder, filename)\n",
    "\n",
    "    print(f\"Downloading {borough} {current_year} data\")\n",
    "    with requests.get(excel_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "print(\"All files downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa278a5f",
   "metadata": {},
   "source": [
    "# NYC Rolling Sales Data Filtering & Unique Address Collection\n",
    "\n",
    "## Filtering Pipeline Summary\n",
    "\n",
    "This cell performs a full cleaning, filtering, and address-preparation pipeline for NYC property sales files (reads from `raw_root_folder`, writes cleaned `.xlsx` files into `interim_root_folder`).\n",
    "\n",
    "### Data Filtering Main steps performed (in order)\n",
    "\n",
    "1. **File discovery & read**\n",
    "   - Iterates year subfolders in `raw_root_folder`.  \n",
    "   - Skips temporary files and non-Excel files (`.xls` / `.xlsx`).\n",
    "   - Reads the file once with `header=None` to detect the header row. The read uses `dtype=object` so columns are not auto-coerced into numeric types.\n",
    "\n",
    "2. **Early illegal-character cleaning**\n",
    "   - Applies `clean_column_name` / `clean_illegal_chars` to object/string cells to remove control characters that will break Excel (`ASCII 0-31`, etc.).\n",
    "   - This prevents `IllegalCharacterError` when saving.\n",
    "\n",
    "3. **Header detection and realignment**\n",
    "   - Normalizes header candidates and matches them against `expected_columns`.\n",
    "   - Re-reads the file once using the detected header row (`header=header_row_idx`) and `dtype=object`, then normalizes column names.\n",
    "\n",
    "4. **Type conversions**\n",
    "   - Converts integer columns (`BLOCK`, `LOT`, `RESIDENTIAL UNITS`, etc.) to pandas `Int64` (nullable integer).\n",
    "   - Converts numeric columns (`LAND SQUARE FEET`, `GROSS SQUARE FEET`, `SALE PRICE`) to numeric (`float`).\n",
    "   - Normalizes string columns (casts to `str`, replaces `\"nan\"` placeholders).\n",
    "   - Parses `SALE DATE` with `pd.to_datetime(..., errors=\"coerce\")`.\n",
    "\n",
    "5. **Residential filtering**\n",
    "   - Normalizes `BUILDING CLASS CATEGORY` strings.\n",
    "   - Keeps only rows that match residential keywords (`FAMILY`, `RENTAL`, `COOP`, `CONDO`, `CONDOP`, `TAX CLASS 1`).\n",
    "\n",
    "6. **Address parsing & cleaning**\n",
    "   - Uses `usaddress` to parse street components and only retains addresses that contain both a house number and a street name.\n",
    "   - Builds `ADDRESS_CLEAN` from parsed components (number + street types/names).\n",
    "\n",
    "7. **Remove non-market transactions**\n",
    "   - Drops rows where `SALE PRICE` ≤ 0 (these are typically non-arms-length or administrative transfers, not market sales).\n",
    "\n",
    "8. **Build `FULL_ADDRESS` for geocoding**\n",
    "   - Maps `BOROUGH` → appropriate city label for geocoders (Manhattan → `New York`, Brooklyn → `Brooklyn`, Queens → `Queens`, Bronx → `Bronx`, Staten Island → `Staten Island`).\n",
    "   - Assembles `FULL_ADDRESS` as:\n",
    "     ```\n",
    "     [ADDRESS_CLEAN], [CITY], NY, [ZIP CODE]\n",
    "     ```\n",
    "     Example: `123 MAIN ST, BROOKLYN, NY, 11215`.\n",
    "\n",
    "9. **Column selection**\n",
    "   - Keeps only the minimal columns needed for mapping/visualization:\n",
    "     - `FULL_ADDRESS`  \n",
    "     - `SALE PRICE`  \n",
    "     - `SALE DATE`\n",
    "\n",
    "10. **Save & convert**\n",
    "    - Ensures `interim_root_folder/<year>/` exists and saves cleaned data as `.xlsx` into that folder.\n",
    "    - If the original file was `.xls`, the script removes the old `.xls` after saving the cleaned `.xlsx`.\n",
    "\n",
    "### Final output\n",
    "- For each processed raw file, you get a cleaned `.xlsx` in `interim_root_folder/<year>/` containing only:\n",
    "     - `FULL_ADDRESS`  \n",
    "     - `SALE PRICE`  \n",
    "     - `SALE DATE`\n",
    "\n",
    "## Unique Address Collection \n",
    "\n",
    "After processing and cleaning all raw property sales files, we collect the **unique addresses** for geocoding and mapping purposes.\n",
    "\n",
    "1. **Track unique addresses while processing**\n",
    "   - Each cleaned `FULL_ADDRESS` from the current file is added to a Python `set()` to automatically ensure uniqueness:\n",
    "     ```python\n",
    "     unique_addresses.update(df[\"FULL ADDRESS\"])\n",
    "     ```\n",
    "\n",
    "2. **Convert to DataFrame**\n",
    "   - Convert the set of unique addresses into a sorted pandas DataFrame:\n",
    "     ```python\n",
    "     unique_df = pd.DataFrame(sorted(unique_addresses), columns=[\"FULL ADDRESS\"])\n",
    "     ```\n",
    "\n",
    "3. **Assign round-robin groups**\n",
    "   - Assign each address to a deterministic group (useful for batch processing or parallel geocoding):\n",
    "     ```python\n",
    "     n_groups = 6\n",
    "     unique_df[\"GROUP\"] = (unique_df.index % n_groups) + 1\n",
    "     ```\n",
    "\n",
    "4. **Add blank latitude/longitude columns for future geocoding**\n",
    "   - Prepare columns to store geocoded coordinates later:\n",
    "     ```python\n",
    "     unique_df[\"LAT\"] = None\n",
    "     unique_df[\"LON\"] = None\n",
    "     ```\n",
    "\n",
    "5. **Reorder columns**\n",
    "   - Ensure the final column order is consistent:\n",
    "     ```python\n",
    "     unique_df = unique_df[[\"GROUP\", \"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "     ```\n",
    "\n",
    "6. **Save to Excel**\n",
    "   - Save the unique addresses and groups to an Excel file in the `interim_root_folder`:\n",
    "     ```python\n",
    "     geocode_cache_path = os.path.join(interim_root_folder, \"unique_addresses.xlsx\")\n",
    "     unique_df.to_excel(geocode_cache_path, index=False, engine=\"openpyxl\")\n",
    "     ```\n",
    "\n",
    "### Final Output\n",
    "- `unique_addresses.xlsx` containing:\n",
    "  - `GROUP` → deterministic assignment for batch processing\n",
    "  - `FULL ADDRESS` → cleaned, geocodable addresses\n",
    "  - `LAT` / `LON` → blank for now, to be filled later by geocoding\n",
    "- Ensures **all addresses are unique** across years and files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# config\n",
    "# ==============================\n",
    "expected_columns = [\n",
    "    \"BOROUGH\", \"NEIGHBORHOOD\", \"BUILDING CLASS CATEGORY\", \"TAX CLASS AT PRESENT\",\n",
    "    \"BLOCK\", \"LOT\", \"EASE-MENT\", \"BUILDING CLASS AT PRESENT\", \"ADDRESS\",\n",
    "    \"APARTMENT NUMBER\", \"ZIP CODE\", \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\",\n",
    "    \"TOTAL UNITS\", \"LAND SQUARE FEET\", \"GROSS SQUARE FEET\", \"YEAR BUILT\",\n",
    "    \"TAX CLASS AT TIME OF SALE\", \"BUILDING CLASS AT TIME OF SALE\",\n",
    "    \"SALE PRICE\", \"SALE DATE\"\n",
    "]\n",
    "\n",
    "int_cols = [\"BLOCK\", \"LOT\", \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\", \"TOTAL UNITS\", \"YEAR BUILT\"]\n",
    "float_cols = [\"LAND SQUARE FEET\", \"GROSS SQUARE FEET\", \"SALE PRICE\"]\n",
    "str_cols = [\"BOROUGH\", \"NEIGHBORHOOD\", \"BUILDING CLASS CATEGORY\", \"TAX CLASS AT PRESENT\",\n",
    "            \"EASE-MENT\", \"BUILDING CLASS AT PRESENT\", \"ADDRESS\", \"APARTMENT NUMBER\",\n",
    "            \"ZIP CODE\", \"TAX CLASS AT TIME OF SALE\", \"BUILDING CLASS AT TIME OF SALE\"]\n",
    "datetime_cols = [\"SALE DATE\"]\n",
    "\n",
    "residential_keywords = [\n",
    "    \"FAMILY\", \"RENTAL\", \"COOP\", \"CONDO\", \"CONDOP\", \"TAX CLASS 1\"\n",
    "]\n",
    "\n",
    "borough_encoding__to_city_map = {\"1\": \"New York\", \"2\": \"Bronx\", \"3\": \"Brooklyn\", \"4\": \"Queens\", \"5\": \"Staten Island\"}\n",
    "unique_addresses = set()\n",
    "\n",
    "# ==============================\n",
    "# REGEX CLEANING HELPER FUNCTIONS\n",
    "# ==============================\n",
    "def normalize(col):\n",
    "    return str(col).replace('\\n', ' ').replace('\"', '').replace(\"  \", \" \").strip().upper()\n",
    "\n",
    "def clean_column_name(s):\n",
    "    if isinstance(s, str):\n",
    "        # Remove illegal characters (ASCII 0-31 except \\t, \\n, \\r)\n",
    "        s = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_illegal_chars(val, replace_with=\"\"):\n",
    "    \"\"\"Remove Excel-illegal control chars from a string.\n",
    "       replace_with: '' (remove) or ' ' (replace with space)\n",
    "    \"\"\"\n",
    "    if isinstance(val, str):\n",
    "        # remove ASCII 0-31 and 127-159\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', replace_with, val).strip()\n",
    "    return val\n",
    "\n",
    "# ==============================\n",
    "# PROPERTY TYPE CLEANING\n",
    "# ==============================\n",
    "def is_residential(category: str) -> bool:\n",
    "    \"\"\"Check if a category is residential.\"\"\"\n",
    "    if not isinstance(category, str):\n",
    "        return False\n",
    "    category = category.upper()\n",
    "    return any(k in category for k in residential_keywords)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ADDRESS CLEANING\n",
    "# ==============================\n",
    "def clean_address(addr: str) -> str | None:\n",
    "    \"\"\"Try to parse & clean address. Returns cleaned address or None if invalid.\"\"\"\n",
    "    if not isinstance(addr, str) or not addr.strip():\n",
    "        return None\n",
    "\n",
    "    addr = re.sub(r\"\\s+\", \" \", addr.strip().title())\n",
    "\n",
    "    try:\n",
    "        parsed, _ = usaddress.tag(addr)\n",
    "        # Keep only addresses that have street + house number\n",
    "        if \"AddressNumber\" in parsed and \"StreetName\" in parsed:\n",
    "            # Rebuild normalized street address\n",
    "            parts = [\n",
    "                parsed.get(\"AddressNumber\", \"\"),\n",
    "                parsed.get(\"StreetNamePreType\", \"\"),\n",
    "                parsed.get(\"StreetName\", \"\"),\n",
    "                parsed.get(\"StreetNamePostType\", \"\")\n",
    "            ]\n",
    "            clean = \" \".join([p for p in parts if p]).strip()\n",
    "            return clean\n",
    "        else:\n",
    "            return None\n",
    "    except usaddress.RepeatedLabelError:\n",
    "        return None\n",
    "    \n",
    "# ==============================\n",
    "# FULL ADDRESS ASSEMBLY\n",
    "# ==============================\n",
    "def build_full_address(row):\n",
    "    parts = [row[\"ADDRESS_CLEAN\"].strip().title()]\n",
    "\n",
    "    # Add borough→city\n",
    "    city = borough_encoding__to_city_map[row[\"BOROUGH\"]].strip().title()\n",
    "    parts.append(row[\"NEIGHBORHOOD\"].strip().title())\n",
    "    parts.append(city)\n",
    "    parts.append(\"NY\")\n",
    "    parts.append(str(row[\"ZIP CODE\"]).strip().title())\n",
    "\n",
    "    return \", \".join([p for p in parts if p])\n",
    "\n",
    "# ==============================\n",
    "# MAIN PROCESSING LOOP\n",
    "# ==============================\n",
    "for year in os.listdir(raw_root_folder):\n",
    "    year_folder = os.path.join(raw_root_folder, year)\n",
    "    if not os.path.isdir(year_folder):\n",
    "        continue\n",
    "    print(f\"\\n=== Entering year: {year} ===\")\n",
    "\n",
    "    for file in os.listdir(year_folder):\n",
    "\n",
    "        # skip non-Excel files or temp excel files\n",
    "        if not file.lower().endswith((\".xls\", \".xlsx\")) or file.startswith(\"~\"):  \n",
    "            continue\n",
    "\n",
    "        # Read in file (.xls or .xlsx)\n",
    "        file_path = os.path.join(year_folder, file)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, header=None, engine=\"openpyxl\", dtype=object)\n",
    "        except Exception:\n",
    "            df = pd.read_excel(file_path, header=None, engine=\"xlrd\", dtype=object)\n",
    "\n",
    "        # display(df.head(10))\n",
    "\n",
    "        # Clean illegal characters before anything else\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col] = df[col].apply(clean_column_name)\n",
    "\n",
    "        # Detect header row\n",
    "        normalized_expected = [normalize(c) for c in expected_columns]\n",
    "        header_row_idx = None\n",
    "        for i, row in df.iterrows():\n",
    "            normalized_row = [normalize(c) for c in row.values]\n",
    "            matches = sum(col in normalized_expected for col in normalized_row)\n",
    "            if matches >= len(normalized_expected) * 0.7:\n",
    "                header_row_idx = i\n",
    "                break\n",
    "\n",
    "        # read in file, now with proper header alignment\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, header=header_row_idx, engine=\"openpyxl\", dtype=object)\n",
    "        except Exception:\n",
    "            df = pd.read_excel(file_path, header=header_row_idx, engine=\"xlrd\", dtype=object)\n",
    "        df.columns = [normalize(c) for c in df.columns]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # display(df.head(10))\n",
    "        \n",
    "        # Convert cols to proper data types\n",
    "        for col in int_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        for col in float_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        for col in str_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).replace(\"nan\", \"\")\n",
    "\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        # tracking how much data we are removing\n",
    "        before_count = len(df)\n",
    "        \n",
    "        # Cleaning residential categories\n",
    "        df[\"BUILDING CLASS CATEGORY\"] = (\n",
    "            df[\"BUILDING CLASS CATEGORY\"]\n",
    "            .astype(str)\n",
    "            .apply(lambda x: re.sub(r\"\\s+\", \" \", x.strip()))\n",
    "        )\n",
    "        df = df[df[\"BUILDING CLASS CATEGORY\"].apply(is_residential)]\n",
    "\n",
    "        #  Cleaning address\n",
    "        df[\"ADDRESS_CLEAN\"] = df[\"ADDRESS\"].apply(clean_address)\n",
    "        df = df[df[\"ADDRESS_CLEAN\"].notna()]\n",
    "\n",
    "        # Removing $0 sales\n",
    "        df = df[df['SALE PRICE'] > 0]\n",
    "\n",
    "        # Removing 0 ZIP CODE\n",
    "        df = df[df['ZIP CODE'] != \"0\"]\n",
    "\n",
    "        # Borough → city conversion and full address building\n",
    "        df[\"FULL ADDRESS\"] = df.apply(build_full_address, axis=1)\n",
    "\n",
    "        # Keep only relevant columns for mapping/analysis\n",
    "        keep_cols = [\n",
    "            \"FULL ADDRESS\",\n",
    "            \"SALE PRICE\",\n",
    "            \"SALE DATE\",\n",
    "        ]\n",
    "        df = df[[c for c in keep_cols if c in df.columns]]\n",
    "\n",
    "        # Print out % of removed data\n",
    "        after_count = len(df)\n",
    "        filtered_out = before_count - after_count\n",
    "        print(f\"{file}: Kept {after_count:,} rows, filtered out {filtered_out:,} ({filtered_out / before_count:.1%})\")\n",
    "\n",
    "        # Create new file path with .xlsx extension for consistent file types\n",
    "        interim_year_folder = os.path.join(interim_root_folder, year)\n",
    "        os.makedirs(interim_year_folder, exist_ok=True)\n",
    "        new_file_path = os.path.join(interim_year_folder, os.path.splitext(file)[0] + \".xlsx\")\n",
    "\n",
    "        # Clean all object/string columns before saving to Excel\n",
    "        for col in df.select_dtypes(include=\"object\"):\n",
    "            df[col] = df[col].apply(clean_illegal_chars)\n",
    "\n",
    "        # Tracks unique addresses seen\n",
    "        # Will be used later to get lat and longs\n",
    "        unique_addresses.update(df[\"FULL ADDRESS\"])\n",
    "\n",
    "        # Save back to the same Excel file\n",
    "        df.to_excel(new_file_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Done filtering!\")\n",
    "\n",
    "\n",
    "print(\"Creating address geocache data frame\")\n",
    "# Convert to DataFrame\n",
    "addresses_df = pd.DataFrame(sorted(unique_addresses), columns=[\"FULL ADDRESS\"])\n",
    "\n",
    "# Add blank LAT/LON columns\n",
    "addresses_df[\"LAT\"] = None\n",
    "addresses_df[\"LON\"] = None\n",
    "\n",
    "# Reorder columns\n",
    "addresses_df = addresses_df[[\"FULL ADDRESS\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Save CSV\n",
    "geocode_cache_path = os.path.join(interim_root_folder, \"addresses.xlsx\")\n",
    "addresses_df.to_excel(geocode_cache_path, index=False, engine=\"openpyxl\")\n",
    "print(f\"Saved {len(addresses_df)} unique addresses to {geocode_cache_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2108d",
   "metadata": {},
   "source": [
    "## Advanced Address Cleaning & Normalization for Geocoding\n",
    "\n",
    "This step performs a **comprehensive cleaning, normalization, and deduplication** of New York City property addresses. The primary purpose is to **reduce the number of geocoding lookups** by condensing multiple variations of the same address into a single normalized form. Once we have the normalized addresses, we can use them as a **key to append latitude and longitude** to our NYC sales data.\n",
    "\n",
    "### Key Features of the Workflow\n",
    "\n",
    "1. **Initial Cleaning**\n",
    "   - Removes illegal/control characters that can break Excel (`ASCII 0-31`, `\\x7F`).\n",
    "   - Strips stray leading punctuation or brackets (e.g., `(56 Street, Bensonhurst, Brooklyn, NY, 11010)` → `56 Street, Bensonhurst,Brooklyn, NY, 11010`).\n",
    "   - Removes excessive leading zeros in street numbers (e.g., `0000 100th Street` → `100th Street`).\n",
    "   - Normalizes spacing and capitalization.\n",
    "\n",
    "2. **Parsing Components**\n",
    "   - Splits `FULL ADDRESS` into:\n",
    "     - `STREET`\n",
    "     - `CITY`\n",
    "     - `STATE`\n",
    "     - `ZIP`\n",
    "   - This allows fine-grained normalization of street names.\n",
    "\n",
    "3. **Street Name Normalization**\n",
    "   - Maps abbreviations to full street type names:\n",
    "     - `AVE`, `AV`, `AVE.` → `Avenue`\n",
    "     - `ST`, `ST.` → `Street`\n",
    "     - `RD` → `Road`\n",
    "     - `PL` → `Place`\n",
    "     - etc.\n",
    "   - Normalizes street ordinals:\n",
    "     - Numeric-only street numbers like `107` → `107th`\n",
    "     - Correctly handles `1 → 1st`, `2 → 2nd`, `3 → 3rd`, and special cases `11-13 → th`.\n",
    "\n",
    "4. **Normalized Full Address**\n",
    "   - Reconstructs `FULL ADDRESS NORM` as:\n",
    "     ```\n",
    "     [STREET_NORM], [CITY], [STATE], [ZIP]\n",
    "     ```\n",
    "   - Ensures all variations of the same address map to the same normalized form, e.g.:\n",
    "     ```\n",
    "     1 Ascan Ave, Forest Hills, Queens, NY, 11375\n",
    "     1 Ascan Avenue, Forest Hills, Queens, NY, 11375\n",
    "     → 1 Ascan Avenue, Forest Hills, Queens, NY, 11375\n",
    "     ```\n",
    "\n",
    "5. **Detecting Merged Addresses**\n",
    "   - Groups addresses by `FULL ADDRESS NORM`.\n",
    "   - Prints only those groups where **two or more original addresses were mapped to the same normalized address**, allowing us to verify the deduplication process.\n",
    "\n",
    "6. **Deduplication**\n",
    "   - Keeps only **unique normalized addresses**.\n",
    "   - Adds a `GROUP` assignment for round-robin processing (useful for batch geocoding to respect API rate limits).\n",
    "   - Final output contains only the necessary columns:\n",
    "     - `GROUP`\n",
    "     - `FULL ADDRESS` (original, unmodified)\n",
    "     - `FULL ADDRESS NORM`\n",
    "     - `LAT`\n",
    "     - `LON`\n",
    "\n",
    "### Workflow Benefits\n",
    "\n",
    "- **Reduces Geocoding Lookups:** By condensing address variants, we minimize unnecessary queries to the geocoding service.\n",
    "- **Non-Destructive:** The original `FULL ADDRESS` column is preserved, enabling a **reliable join** back to the NYC sales dataset.\n",
    "- **Consistent Normalization:** Handles street type abbreviations, ordinals, leading zeros, stray punctuation, and capitalization variations.\n",
    "- **Traceable Merges:** Any address variants combined into a single normalized form are logged for review.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Once this script is run, we can use the normalized addresses with **geopy's Nominatim** (with `RateLimiter`) to obtain latitude and longitude for each unique normalized address. These coordinates can then be joined back to the full NYC sales data using the original `FULL ADDRESS` as the key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de79189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "street_map = {\n",
    "    r\"\\bAVE\\b\\.?\": \"Avenue\",\n",
    "    r\"\\bAV\\b\\.?\": \"Avenue\",\n",
    "    r\"\\bST\\b\\.?\": \"Street\",\n",
    "    r\"\\bRD\\b\\.?\": \"Road\",\n",
    "    r\"\\bPL\\b\\.?\": \"Place\",\n",
    "    r\"\\bTER\\b\\.?\": \"Terrace\",\n",
    "    r\"\\bBLVD\\b\\.?\": \"Boulevard\",\n",
    "    r\"\\bLN\\b\\.?\": \"Lane\",\n",
    "    r\"\\bDR\\b\\.?\": \"Drive\",\n",
    "    r\"\\bCT\\b\\.?\": \"Court\",\n",
    "    r\"\\bHWY\\b\\.?\": \"Highway\",\n",
    "    r\"\\bPKWY\\b\\.?\": \"Parkway\",\n",
    "    r\"\\bSQ\\b\\.?\": \"Square\",\n",
    "    r\"\\bCTR\\b\\.?\": \"Center\",\n",
    "}\n",
    "n_groups = 6\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "def clean_illegal_chars(val):\n",
    "    \"\"\"Remove illegal Excel characters from string cells.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        return re.sub(r'[\\x00-\\x1F\\x7F]', '', val)\n",
    "    return val\n",
    "\n",
    "def clean_address_text(addr: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up weird formatting in addresses:\n",
    "    - Remove stray leading punctuation/brackets.\n",
    "    - Remove excessive leading zeros in street numbers (e.g., 0000 100TH -> 100TH).\n",
    "    - Normalize spaces and capitalization.\n",
    "    \"\"\"\n",
    "    if not isinstance(addr, str):\n",
    "        return addr\n",
    "\n",
    "    # Remove illegal and control characters first\n",
    "    addr = clean_illegal_chars(addr)\n",
    "\n",
    "    # Strip leading punctuation or symbols (like (, \", ', etc.)\n",
    "    addr = re.sub(r'^[^\\w\\d]+', '', addr)\n",
    "\n",
    "    # Fix excessive leading zeros at the beginning of street numbers\n",
    "    # e.g., \"0000 100TH STREET\" -> \"100TH STREET\"\n",
    "    addr = re.sub(r'^\\s*0+\\s*(?=\\d)', '', addr)\n",
    "\n",
    "    # Remove double spaces\n",
    "    addr = re.sub(r'\\s{2,}', ' ', addr)\n",
    "\n",
    "    # Strip trailing/leading whitespace\n",
    "    addr = addr.strip()\n",
    "\n",
    "    # Title formatting for captialization\n",
    "    addr = addr.title()\n",
    "\n",
    "    return addr\n",
    "\n",
    "def normalize_street_ordinal(street_name):\n",
    "    \"\"\"Normalize street ordinals like 107 → 107TH, 1 → 1ST, 2 → 2ND, etc.\"\"\"\n",
    "    tokens = street_name.split()\n",
    "    new_tokens = []\n",
    "    for t in tokens:\n",
    "        if re.fullmatch(r\"\\d+\", t):  # purely numeric\n",
    "            n = int(t)\n",
    "            if 10 <= n % 100 <= 20:\n",
    "                suffix = \"th\"\n",
    "            else:\n",
    "                suffix = {1: \"st\", 2: \"nd\", 3: \"rd\"}.get(n % 10, \"th\")\n",
    "            new_tokens.append(f\"{t}{suffix}\")\n",
    "        else:\n",
    "            new_tokens.append(t)\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def normalize_street_name(street):\n",
    "    \"\"\"Normalize capitalization, abbreviations, and ordinals in street names.\"\"\"\n",
    "    s = str(street).strip().title()\n",
    "    for pattern, repl in street_map.items():\n",
    "        s = re.sub(pattern, repl, s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = normalize_street_ordinal(s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_full_address(addr):\n",
    "    \"\"\"Parse 'FULL ADDRESS' into [street, neighborhood, city, state, zip].\"\"\"\n",
    "    parts = [p.strip() for p in str(addr).split(\",\")]\n",
    "    if len(parts) != 5:\n",
    "        return [None, None, None, None]\n",
    "    street, neighborhood, city, state, zip_code = parts[0], parts[1], parts[2], parts[3], parts[4]\n",
    "    return [street, neighborhood, city, state, zip_code]\n",
    "\n",
    "# =====================================================\n",
    "# MAIN PROCESSING\n",
    "# =====================================================\n",
    "address_cache_path = os.path.join(interim_root_folder, \"addresses.xlsx\")\n",
    "address_cache_path_remapped = os.path.join(interim_root_folder, \"addresses_condensed.xlsx\")\n",
    "\n",
    "address_df = pd.read_excel(address_cache_path, engine=\"openpyxl\")\n",
    "\n",
    "# Clean Address \n",
    "address_df[\"CLEANED FULL ADDRESS\"] = address_df[\"FULL ADDRESS\"].apply(clean_address_text)\n",
    "\n",
    "# Split into components\n",
    "address_df[[\"STREET\", \"NEIGHBORHOOD\", \"CITY\", \"STATE\", \"ZIP\"]] = address_df[\"CLEANED FULL ADDRESS\"].apply(\n",
    "    lambda x: pd.Series(parse_full_address(x))\n",
    ")\n",
    "address_df[\"STATE\"] = address_df[\"STATE\"].str.upper()\n",
    "\n",
    "# Normalize street names\n",
    "address_df[\"STREET_NORM\"] = address_df[\"STREET\"].apply(normalize_street_name)\n",
    "\n",
    "# Build normalized full address\n",
    "address_df[\"FULL ADDRESS NORM\"] = (\n",
    "    address_df[\"STREET_NORM\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"NEIGHBORHOOD\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"CITY\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"STATE\"].astype(str)\n",
    "    + \", \"\n",
    "    + address_df[\"ZIP\"].astype(str)\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# SHOW ONLY MERGED ADDRESSES (duplicates after cleaning)\n",
    "# =====================================================\n",
    "grouped = (\n",
    "    address_df.groupby(\"FULL ADDRESS NORM\")[\"FULL ADDRESS\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "merged = grouped[grouped[\"FULL ADDRESS\"].apply(lambda lst: len(lst) > 1)]\n",
    "\n",
    "\n",
    "print(\"=== ADDRESSES THAT GOT MERGED TO THE SAME NORMALIZED FORM ===\")\n",
    "for _, row in merged.iterrows():\n",
    "    print(f\"\\n→ Normalized: {row['FULL ADDRESS NORM']}\")\n",
    "    for orig in row[\"FULL ADDRESS\"]:\n",
    "        print(f\"   - {orig}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# KEEP ONLY UNIQUE NORMALIZED ADDRESSES\n",
    "# =====================================================\n",
    "unique_address_df = address_df.drop_duplicates(subset=[\"FULL ADDRESS NORM\"]).reset_index(drop=True)\n",
    "\n",
    "# Assign groups round-robin\n",
    "unique_address_df[\"GROUP\"] = (unique_address_df.index % n_groups) + 1\n",
    "\n",
    "# Keeping only neccessary columns\n",
    "unique_address_df = unique_address_df[[\"GROUP\", \"FULL ADDRESS\", \"FULL ADDRESS NORM\", \"LAT\", \"LON\"]]\n",
    "\n",
    "# Printing out how much data we re-mapped \n",
    "before_count = len(address_df)\n",
    "after_count = len(unique_address_df)\n",
    "filtered_out = before_count - after_count\n",
    "print(f\"Kept {after_count:,} unique and normalized addresses, condensed down {filtered_out:,} addresses ({filtered_out / before_count:.1%})\")\n",
    "\n",
    "# Save to Excel (includes both columns for traceability)\n",
    "unique_address_df.to_excel(address_cache_path_remapped, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"\\n Saved cleaned, deduplicated addresses to: {address_cache_path_remapped}\")\n",
    "display(unique_address_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46084c7",
   "metadata": {},
   "source": [
    "### Creating Coordinate Columns from Addresses\n",
    "\n",
    "`GeoJSON` files typically utilize coordinate lat. and long. in order to be abl to render maps and other layouts. For compatability, the `ADDRESS` listed in our data need to have an associated coordinate location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# config\n",
    "# =====================================================\n",
    "config = {\n",
    "    \"input_file\": \"addresses_condensed.xlsx\",\n",
    "    \"group_number\": 1,  # <-- each team member sets their group\n",
    "    \"user_agent\": \"nyc_affordability_transit_access\",\n",
    "    \"min_delay_seconds\": 1.0,\n",
    "    \"flush_every\": 500,  # how many rows before writing to disk\n",
    "    \"max_retries\": 3,\n",
    "}\n",
    "# NYC bounding box (roughly 5 boroughs)\n",
    "nyc_viewbox = (-74.25909, 40.477399, -73.700272, 40.917577)\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "\n",
    "def safe_write_excel(df, final_path):\n",
    "    \"\"\"Safely write Excel file without risk of corruption on interruption.\"\"\"\n",
    "    tmp_dir = tempfile.gettempdir()\n",
    "    tmp_path = os.path.join(tmp_dir, f\"tmp_{os.path.basename(final_path)}\")\n",
    "    try:\n",
    "        # Write to a temporary file first\n",
    "        df.to_excel(tmp_path, index=False, engine=\"openpyxl\")\n",
    "        # Atomically replace the old file\n",
    "        shutil.move(tmp_path, final_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Failed to safely write {final_path}: {e}\")\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "\n",
    "def geocode_address(geolocator, address_json):\n",
    "    \"\"\"Call Nominatim geocode with NYC-focused parameters.\"\"\"\n",
    "    return geolocator.geocode(\n",
    "        address_json,\n",
    "        country_codes=\"us\",         # restrict to USA\n",
    "        addressdetails=True,        # return structured address info\n",
    "        language=\"en\",              # English results\n",
    "    )\n",
    "\n",
    "def geocode_with_retries(geocode_func, address, max_retries=3):\n",
    "    \"\"\"Take a single comma-separated address string, and retry if failed.\"\"\"\n",
    "    parts = [p.strip() for p in address.split(\",\")]\n",
    "    address = f\"{parts[0]}, {parts[3]}, {parts[4]}\"\n",
    "    attempt = 0\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            return geocode_func(address)\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Warn: geocode error ({attempt}/{max_retries}) for '{address}': {e}. Backing off {wait}s.\")\n",
    "            time.sleep(wait)\n",
    "    return None\n",
    "\n",
    "# =====================================================\n",
    "# MAIN SCRIPT\n",
    "# =====================================================\n",
    "input_path = os.path.join(interim_root_folder, config[\"input_file\"])\n",
    "df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "\n",
    "# Filter to only the given group and only rows without lat/lon\n",
    "to_geocode_df = df[\n",
    "    (df[\"GROUP\"] == config[\"group_number\"]) &\n",
    "    (df[\"LAT\"].isna() | df[\"LON\"].isna())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Group {config['group_number']} has {len(to_geocode_df)} addresses to geocode.\")\n",
    "\n",
    "# Initialize geolocator with NYC-specific parameters\n",
    "geolocator = Nominatim(user_agent=config[\"user_agent\"], timeout=10)\n",
    "\n",
    "# Use a partial so each call applies NYC bounding box parameters\n",
    "geocode = RateLimiter(\n",
    "    partial(geocode_address, geolocator),\n",
    "    min_delay_seconds=config[\"min_delay_seconds\"],\n",
    "    error_wait_seconds=10\n",
    ")\n",
    "\n",
    "# Prepare per-group output file\n",
    "group_cache_file = os.path.join(\n",
    "    interim_root_folder, f\"geocode_group_{config['group_number']}.xlsx\"\n",
    ")\n",
    "\n",
    "# Load existing cache if available\n",
    "if os.path.exists(group_cache_file):\n",
    "    cache_df = pd.read_excel(group_cache_file, engine=\"openpyxl\")\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=[\"GROUP\",\"FULL ADDRESS\",\"FULL ADDRESS NORM\",\"LAT\",\"LON\"])\n",
    "\n",
    "# Build a set of already geocoded addresses (FULL ADDRESS NORM) for quick lookup\n",
    "cache_key = set(cache_df[\"FULL ADDRESS NORM\"].astype(str).tolist())\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "for idx, row in tqdm(to_geocode_df.iterrows(), total=len(to_geocode_df), desc=\"Geocoding\"):\n",
    "    norm_addr = str(row[\"FULL ADDRESS NORM\"])\n",
    "    \n",
    "    if norm_addr in cache_key:\n",
    "        continue  # already geocoded\n",
    "    \n",
    "    try:\n",
    "        loc = geocode_with_retries(lambda a: geocode(a), norm_addr, max_retries=config[\"max_retries\"])\n",
    "        lat, lon = (loc.latitude, loc.longitude) if loc else (None, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding '{norm_addr}': {e}\")\n",
    "        lat, lon = None, None\n",
    "\n",
    "    new_rows.append({\n",
    "        \"GROUP\": row[\"GROUP\"],\n",
    "        \"FULL ADDRESS\": row[\"FULL ADDRESS\"],\n",
    "        \"FULL ADDRESS NORM\": row[\"FULL ADDRESS NORM\"],\n",
    "        \"LAT\": lat,\n",
    "        \"LON\": lon,\n",
    "    })\n",
    "\n",
    "    # Flush periodically\n",
    "    if len(new_rows) >= config[\"flush_every\"]:\n",
    "        tmp_df = pd.DataFrame(new_rows)\n",
    "        cache_df = pd.concat([cache_df, tmp_df], ignore_index=True)\n",
    "        safe_write_excel(cache_df, group_cache_file)\n",
    "        print(f\"Flushed {len(new_rows)} rows to {group_cache_file} (total cached: {len(cache_df)})\")\n",
    "        new_rows = []\n",
    "\n",
    "# Final flush\n",
    "if new_rows:\n",
    "    tmp_df = pd.DataFrame(new_rows)\n",
    "    cache_df = pd.concat([cache_df, tmp_df], ignore_index=True)\n",
    "    safe_write_excel(cache_df, group_cache_file)\n",
    "    print(f\"Final flush: {len(new_rows)} rows written to {group_cache_file} (total cached: {len(cache_df)})\")\n",
    "\n",
    "print(f\"\\nGeocoding complete for group {config['group_number']}. Saved results to: {group_cache_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NYC-Affordability-Transit-Access",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
